ControlMachine={{ groups['controller'][0] }}
#ControlAddr=127.0.0.1

AuthType=auth/munge
ClusterName={{ cluster }}
CryptoType=crypto/munge
FastSchedule=1
JobAcctGatherType=jobacct_gather/none
JobCompType=jobcomp/none
MpiDefault=none
ProctrackType=proctrack/pgid
ReturnToService=2
SallocDefaultCommand="/opt/slurm/17.11.1-2/bin/srun -n1 -N1 --pty --preserve-env --mpi=none $SHELL"
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_CPU
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldPidFile=/var/log/slurm/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/log/slurm/slurmd.pid
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser={{ slurm_user }}
SlurmdUser=root
StateSaveLocation=/var/log/slurm/state
SwitchType=switch/none
SrunPortRange=60000-63000


AccountingStorageType=accounting_storage/filetxt
AccountingStorageLoc=/var/log/slurm/accounting
#AccountingStorageHost={{ groups['controller'][0] }}

# 
# COMPUTE NODES
#
# This template is for clusters with non-uniform number of cpus per node.


{% for wn in workers_params %}
NodeName={{ wn.nodename }} CPUs={{ wn.cpus_per_node }} State=UNKNOWN
{% endfor %}


PartitionName={{ partition_name }} Nodes={% for wn in workers_params -%}{{ wn.nodename }}{% if not loop.last -%}
,{% endif %}{% endfor %} Default=YES MaxTime=INFINITE State=UP
